{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "acb01db1-ed72-4f44-9f9b-2d4415e74fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ENV(object):\n",
    "    def __init__(self):\n",
    "        self.terminal = False\n",
    "        self.state = 0 #0代表状态相遇，１代表状态卖拐。\n",
    "        \n",
    "    def reset(self):\n",
    "        self.terminal = False\n",
    "        self.state = 0\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        '''根据action返回reward和新的state\n",
    "           action为0代表攀谈，1代表离开。\n",
    "        '''\n",
    "        if self.state == 0: #相遇\n",
    "            if action==0: #攀谈\n",
    "                self.reward = 0\n",
    "                self.state = 1\n",
    "            else: #离开\n",
    "                self.reward = 10\n",
    "                self.terminal = True\n",
    "        else: #卖拐\n",
    "            if action==0: #攀谈\n",
    "                self.reward = -100\n",
    "                self.terminal = True\n",
    "            else: #离开\n",
    "                self.reward = 100\n",
    "                self.terminal = True\n",
    "            \n",
    "        return self.state, self.reward, self.terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a6cd836b-ff47-4bf3-95a4-fc36f010ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.state_dim = state_dim #状态空间大小，有多少个状态就是几。\n",
    "        self.action_dim = action_dim #动作空间大小，有多少个动作就是几。\n",
    "        self.alpha = learning_rate #学习率，对应公式里的alpha\n",
    "        self.gamma = gamma #累加因子对应公式里的gamma\n",
    "        self.epsilon = epsilon #冒险系数，对应公式里的epsilon\n",
    "        self.Q = np.zeros([state_dim, action_dim]) #Q表格，有多少个状态就有多少行，有多少个动作就有多少列。\n",
    "        \n",
    "    def getAction(self, current_state):\n",
    "        random_p = np.random.uniform(0,1) #在标准高斯分布上随机选一个值\n",
    "        \n",
    "        if random_p<(1-self.epsilon): #如果随机值小于1-epsilon，则根据Q表格查询在当前状态下的最优动作。\n",
    "            action = self.getActionByQ(current_state)\n",
    "        else: #否则，随机选取一个动作。\n",
    "            action = np.random.choice(self.action_dim)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def getActionByQ(self, current_state):\n",
    "        '''\n",
    "        根据状态，查Q表格获得最优action\n",
    "        '''\n",
    "        Q_row = self.Q[current_state] #从Q表格中选出目前状态对应的行。\n",
    "        maxQ = np.max(Q_row) #获得改行的最大值。\n",
    "        actions = np.where(Q_row==maxQ)[0] #将所有最大值所对应的列号组成动作集合。\n",
    "        action = np.random.choice(actions) #在动作集合中随机选取一个动作。\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, current_state, action, reward, next_state, next_action, terminal):\n",
    "        '''\n",
    "        Q(S_t,A_t) <- Q(S_t,A_t) + alph[R_t+1+gammaQ(S_t+1,A_t+1)-Q(S_t,A_t)]\n",
    "        根据算法中的更新公式，更新Q表格中，当前状态（行），当前action（列）的对应值Q_sa\n",
    "        '''\n",
    "        Q_sa = self.Q[current_state][action] #当前Q值\n",
    "        Q_sa_next = self.Q[next_state][next_action] #下一时刻Q值\n",
    "        if terminal: #episode结束时，更新Q值\n",
    "            self.Q[current_state][action] = Q_sa + self.alpha*(reward-Q_sa)\n",
    "        else: #更新Q值\n",
    "            self.Q[current_state][action] = Q_sa + self.alpha*(reward + self.gamma*Q_sa_next-Q_sa)\n",
    "            \n",
    "    def showQTable(self):\n",
    "        print(self.Q)\n",
    "        \n",
    "class QLAgent(SarsaAgent):\n",
    "    def learn(self, current_state, action, reward, next_state, next_action, terminal):\n",
    "        '''\n",
    "        Q(S_t,A_t) <- Q(S_t,A_t) + alph[R_t+1+gammaQ(S_t+1,A_t+1)-Q(S_t,A_t)]\n",
    "        根据算法中的更新公式，更新Q表格中，当前状态（行），当前action（列）的对应值Q_sa\n",
    "        '''\n",
    "        Q_sa = self.Q[current_state][action] #当前Q值\n",
    "        Q_s_next = self.Q[next_state] #下一状态Q值\n",
    "        if terminal: #episode结束时，更新Q值\n",
    "            self.Q[current_state][action] = Q_sa + self.alpha*(reward-Q_sa)\n",
    "        else: #更新Q值\n",
    "            self.Q[current_state][action] = Q_sa + self.alpha*(reward + self.gamma*np.max(Q_s_next)-Q_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "11cd47bb-2aa4-407e-b219-228aa5322a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "Episode 10, Total reward 10\n",
      "[[0.        6.5132156]\n",
      " [0.        0.       ]]\n",
      "Episode 20, Total reward 10\n",
      "[[  0.           8.64914828]\n",
      " [-10.           0.        ]]\n",
      "Episode 30, Total reward 10\n",
      "[[  0.           9.52898713]\n",
      " [-10.           0.        ]]\n",
      "Episode 40, Total reward 10\n",
      "[[  0.           9.83576797]\n",
      " [-10.           0.        ]]\n",
      "Episode 50, Total reward 10\n",
      "[[  0.           9.94273583]\n",
      " [-10.           0.        ]]\n",
      "Episode 60, Total reward 10\n",
      "[[  0.           9.97781469]\n",
      " [-10.          10.        ]]\n",
      "Episode 70, Total reward 10\n",
      "[[  0.9          9.99140496]\n",
      " [-10.          19.        ]]\n",
      "Episode 80, Total reward 10\n",
      "[[  0.9          9.99700309]\n",
      " [-10.          19.        ]]\n",
      "Episode 90, Total reward 10\n",
      "[[  2.52         9.99883894]\n",
      " [-10.          27.1       ]]\n",
      "Episode 100, Total reward 10\n",
      "[[  2.52         9.99959516]\n",
      " [-10.          27.1       ]]\n",
      "Episode 110, Total reward 10\n",
      "[[  4.707        9.99984316]\n",
      " [-10.          34.39      ]]\n",
      "Episode 120, Total reward 10\n",
      "[[  4.707        9.99994531]\n",
      " [-10.          34.39      ]]\n",
      "Episode 130, Total reward 100\n",
      "[[ 13.472496     9.99997384]\n",
      " [-10.          52.17031   ]]\n",
      "Episode 140, Total reward 100\n",
      "[[ 41.93396774   9.99997384]\n",
      " [-19.          81.46979811]]\n",
      "Episode 150, Total reward 100\n",
      "[[ 66.77932896   9.99997384]\n",
      " [-19.          93.53891811]]\n",
      "Episode 160, Total reward 100\n",
      "[[ 78.75099632   9.99997646]\n",
      " [-19.          97.4968445 ]]\n",
      "Episode 170, Total reward 100\n",
      "[[ 84.76910914   9.99997881]\n",
      " [-19.          99.03022627]]\n",
      "Episode 180, Total reward 100\n",
      "[[ 75.54724134   9.99998093]\n",
      " [-34.39        99.53616023]]\n",
      "Episode 190, Total reward 100\n",
      "[[ 84.79890373   9.99998093]\n",
      " [-34.39        99.83826907]]\n"
     ]
    }
   ],
   "source": [
    "def episode(env, LF):\n",
    "    '''\n",
    "    agent与ENV互动一个episode，\n",
    "    在此过程中对Q表格进行更新，直到episode结束。\n",
    "    env: ENV对象\n",
    "    LF： 老范的拼音缩写， Agent的对象\n",
    "    '''\n",
    "    total_reward = 0 #一个episode获得的总reward\n",
    "    state = env.reset() #初始化ENV\n",
    "    action = LF.getAction(state) #根据state获得action\n",
    "    \n",
    "    while True:\n",
    "        next_state, reward, terminal = env.step(action) #用action跟ENV互动，获得下一个状态，reward和是否episode结束标志符\n",
    "        next_action = LF.getAction(next_state) #根据下一个state获得下一个action\n",
    "        LF.learn(state,action,reward,next_state,next_action,terminal) #更新Q表格\n",
    "        total_reward += reward #累加reward\n",
    "        action = next_action #准备下一次互动，更新action\n",
    "        state = next_state #准备下一次互动，更新state\n",
    "\n",
    "        if terminal: #到达终点则结束循环\n",
    "            break\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "env = ENV()\n",
    "###以下算法二选一，注释掉不用的那行###\n",
    "LF = SarsaAgent(2,2) #这里让老范具有sarsa算法的思维\n",
    "# LF = QLAgent(2,2) #这里让老范具有Q-learning散发的思维\n",
    "\n",
    "LF.showQTable()\n",
    "\n",
    "for ep in range(1, 200):   \n",
    "    total_reward = episode(env, LF)\n",
    "    if ep%10==0:\n",
    "        print(\"Episode {}, Total reward {}\".format(ep, total_reward))\n",
    "        LF.showQTable()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68069967-c64a-495d-8100-2833f4064c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
