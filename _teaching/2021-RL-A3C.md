---
title: "A3C算法"
collection: teaching
type: "强化学习教程-4"
permalink: /teaching/2021-RL-4
venue: "杜新宇,京东"
date: 2021-06-03
location: "中国, 北京"
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 1.A3C-**Asynchronous Advantage Actor-Critic**

A3C算法是Actor-Critic算法的并行版，**Critic**是一个价值模型，学习价值函数，多个**Actor**策略模型并行的学习策略函数，并时不时的和全局参数进行同步。以状态价值函数为例，损失函数可以写成累加收益与状态价值的方差形式$$J_v(w)=(G_t-V_w(s))^2$$，用梯度下降法学习参数$$w$$。然后，用价值函数当做基础值去更新策略函数的参数$$\theta$$。

&emsp;**Critic**：参数为$$w$$的价值模型，可以是动作价值函数$$Q_w(a\mid s)$$或者状态价值函数$$V_w(s)$$。负责用来评价策略的好坏。

&emsp;**Actor**：参数为$$\theta$$的策略模型。负责输出动作。

## 2.算法逻辑

1.随机初始化全局的策略模型参数和价值模型参数$$\theta,w$$；并使各线程的参数保持一致，$$\theta',w'$$

2.初始化时间步长$$t=1$$

2.While $$T\leq T_{MAX}$$:

&emsp;1.重置梯度$$d\theta=0,dw=0$$

&emsp;2.将全局参数同步到各线程$$\theta'=\theta,w'=w$$

&emsp;3.令$$t_{start}=t,$$，并随机生成一个初始状态$$s_t$$

&emsp;4.While $$s_t!=TERMINAL$$ 且 $$t-t_{start}\le t_{max}$$:

&emsp;&emsp;1.通过策略函数生成一个行动$$A_t\sim \pi_\theta'(A_t\mid S_t)$$，获得一个收益$$R_t$$和新状态$$s_{t+1}$$

&emsp;&emsp;2.更新$$t=t+1, T=T+1$$

&emsp;5.初始化收益函数
$$
R=\left\{
\begin{split}
&0 \qquad if \ s_t \ is \ TERMINAL \\
&V_{w'}(s_t) \qquad otherwise
\end{split}
\right.
$$

&emsp;6.For $$i=t-1,...,t_{start}$$:

&emsp;&emsp;1.$$R\leftarrow \gamma R+R_i$$

&emsp;&emsp;2.累加各线程的梯度$$d\theta\leftarrow d\theta + \nabla_{\theta'}log\pi_{\theta'}(a_i\mid s_i)(R-V_{w'}(s_i))$$

&emsp;&emsp;累加各线程的梯度$$dw\leftarrow dw + 2(R-V_{w'}(s_i))\nabla_{w'}(R-V_{w'}(s_i))$$

&emsp;7.用累加梯度异步更新全局参数$$\theta,w$$